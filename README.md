# Knowledge Distillation Net wirh Swish
[Here](https://docs.google.com/document/d/1kWkoZbcsIklhWM9ORHZY4a1gSNI69jdmjC_t5Db1x78/edit?usp=sharing) is the link to the google doc report. Main files you may be interested in are the following:
- student.py: student network, training and testing with ReLu
- student_swish.py: student network, training and testing with Swish
- resnet.py: teacher network, training and testing with Swish
- swish_function.cpp: C++ implementation of the Swish Activation Function
- autograd_check.py: check the Swish implementation with torch.autograd.gradcheck
- kd.py: student network, training and testing with Swish and KD (both on the fly and disk caching)
- plots/: all the plots of the loss, top1 and top5 accuracy generated by plot.py
- log/: all the training and evaluation logs
